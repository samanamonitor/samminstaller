services:
  minio:
    image: minio/minio
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1 # If data will be store in AWS S3, this number must be set to 0
    entrypoint: [""]
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5
    command: 
      - "sh"
      - "-c"
      - "mkdir -p /data/$${S3_MIMIR_BUCKET_NAME} && mkdir -p /data/$${S3_LOKI_BUCKET_NAME} && minio server --quiet /data"
    env_file:
      - ./config/env/minio.env
    volumes:
      - minio-data:/data

  db:
    build: ./db
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    env_file: 
      - ./config/env/mysql.env
      - ./config/env/grafana.env
    healthcheck:
        test: ["CMD-SHELL", "sh -c /samm/test.sh"]
        start_period: 10s
        start_interval: 5s
        interval: 30s
        timeout: 10s
        retries: 5
    volumes:
      - mysql-data:/var/lib/mysql

  mimir:
    image: grafana/mimir:latest
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    command:
      - -config.file=/etc/mimir.yaml
      - -config.expand-env=true
    pull_policy: always
    env_file:
      - ./config/env/minio.env
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./config/mimir/mimir.yaml:/etc/mimir.yaml
      - mimir-data:/data

  loki:
    image: grafana/loki:latest
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    env_file:
      - ./config/env/minio.env
    command:
      - -config.expand-env=true
      - -config.file=/etc/loki/local-config.yaml
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./config/loki/local-config.yaml:/etc/loki/local-config.yaml

  prometheus:
    image: prom/prometheus:latest
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --log.level=error
      - --storage.tsdb.path=/prometheus
    depends_on:
      mimir:
        condition: service_started
    volumes:
      - ./config/prometheus/prometheus.yaml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus

  # Alloy is a service that provides a unified interface for managing and querying data from various sources
  # It is designed to work with Grafana and Prometheus, allowing users to visualize and analyze data from multiple systems in a single dashboard
  # Used to process syslog messages and SNMP Traps, label them, and send them lo loki
  syslog:
    image: grafana/alloy:latest
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    command: run --server.http.listen-addr=0.0.0.0:12345 --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy
    depends_on:
      - loki
      - prometheus
    ports:
    # - "12345:12345" - Not published by default, can be accessed via Grafana
      - "51893:51893/udp"
      - "51894:51894/udp"
    volumes:
      - ./config/alloy/alloy.conf:/etc/alloy/config.alloy
      - mimir-data:/data
  
  # This service captures SNMP Traps and converts them to syslog messages, to be forwarded to Alloy
  snmptrapd:
    build: ./snmptrapd
    hostname: SNMPTraps # Hostname in grafana labels
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    pull_policy: always
    env_file:
      - ./config/env/snmptrapd.env
    ports:
      - "162:162/udp"
    volumes:
      - ./config/mibs:/mibs
      - ./config/snmptrapd/snmptrapd.conf:/conf/snmptrapd.conf
      - ./config/snmptrapd/rsyslog.conf:/conf/rsyslog.conf

  grafana:
    build: ./grafana
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    pull_policy: always
    env_file:
      - ./config/env/grafana.env
    depends_on:
      db:
        condition: service_healthy
      mimir:
        condition: service_started
      loki:
        condition: service_started
    volumes:
      - ./config/grafana:/etc/grafana
      - ./config/kerberos/krb5.conf:/etc/krb5.conf
      - ./config/dashboards:/var/lib/grafana/dashboards
      - ./config/grafana-plugins/grafana-dashboardreporter-app:/var/lib/grafana/plugins/grafana-dashboardreporter-app #Plugin for PDF Reports
      #- ./config/grafana-plugins/samm-citrixodata-datasource:/var/lib/grafana/plugins/samm-citrixodata-datasource #Plugin for Citrix Cloud queries
  
  # Adding Grafana-image-renderer required for Reporting
  renderer:
    image: grafana/grafana-image-renderer:latest
    depends_on:
      - grafana
    environment:
      # Recommendation of grafana-image-renderer for optimal performance
      # https://grafana.com/docs/grafana/latest/setup-grafana/image-rendering/#configuration
      - RENDERING_MODE=clustered
      - RENDERING_CLUSTERING_MODE=context
      - RENDERING_CLUSTERING_MAX_CONCURRENCY=5
      - RENDERING_CLUSTERING_TIMEOUT=60
      - IGNORE_HTTPS_ERRORS=true
      # - RENDERING_ARGS=--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-accelerated-2d-canvas,--disable-gpu,--remote-debugging-port=9090

  #Adding headless Chrome required for reporting
  chrome:
    image: chromedp/headless-shell:latest
    shm_size: 2G
    init: true

  load-balancer:
    image: nginx:latest
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    volumes:
      - ./config/nginx:/etc/nginx:ro
    ports:
      - 80:80
      - 443:443
    depends_on:
      grafana:
        condition: service_started
      samm-manager:
        condition: service_started

  samm-host:
    image: quay.io/prometheus/node-exporter:latest
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  samm-server:
    build: ./sammbase
    image: sammbase
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 1
    command: [ "/usr/local/bin/server.py", "/usr/local/samm/etc/conf.json" ]
    env_file:
      - ./config/env/samm.env
    environment:
      - PYTHONPATH=/app
    working_dir: /app
    volumes:
      - ./config/samm:/usr/local/samm/etc
      - ./config/kerberos/krb5.conf:/etc/krb5.conf
      - ./config/mibs:/mibs

  samm-manager:
    build: ./sammmanager
    restart: on-failure:3
    hostname: samm-manager
    deploy:
      mode: replicated
      replicas: 1
    volumes:
      - ./config/manager/conf.json:/app/conf.json

  samm-test:
    image: sammbase
    command: [ "/bin/bash" ]
    hostname: samm-test
    stdin_open: true
    tty: true
    deploy:
      mode: replicated
      replicas: 0
    volumes:
      - /usr/src:/usr/src
      - ./config/samm:/usr/local/samm/etc
      - ./config/kerberos/krb5.conf:/etc/krb5.conf
      - ./config/mibs:/mibs

  # repeat this service for each pool
  # if each pool uses the same username and password, then use the sammxen.env
  # remember to replace poolmaster_ip with the pool master IP
  samm-xen-1:
    build: ./sammxen
    image: samm-xen
    restart: on-failure:3
    deploy:
      mode: replicated
      replicas: 0
    env_file:
      - ./config/env/sammxen.env
    command: [ "/app/config.json" ]
    environment:
      XEN_HOST: "poolmaster_ip"
    volumes:
      - ./config/sammxen/config.json:/app/config.json

# VmWare Monitoring
# * enable the following section only if VMWare is to be monitored
#  vmware-collector:
#    image: pryorda/vmware_exporter
#    restart: on-failure:3
#    hostname: vmware-collector
#    environment:
#      VSPHERE_IGNORE_SSL: True
#      VSPHERE_SPECS_SIZE: 2000
#    deploy:
#      mode: replicated
#      replicas: 0
#    volumes:
#      - ./config/vmware-collector/config.yml:/samm/config.yml
#

# Settings for future versions of SAMM
#
#  mq:
#    image: rabbitmq:latest
#    restart: on-failure:3
#    deploy:
#      mode: replicated
#      replicas: 0
#    hostname: mq
#    healthcheck:
#      test: ["CMD", "rabbitmqctl", "status"]
#      interval: 10s
#      timeout: 5s
#      retries: 3
#
#  samm-scheduler:
#    build: ./sammbase
#    deploy:
#      mode: replicated
#      replicas: 0
#    command: [ "/usr/local/bin/scheduler.py", "/usr/local/samm/etc/conf.json" ]
#    hostname: samm-scheduler
#    env_file:
#      - ./config/env/samm.env
#    depends_on:
#      mq:
#        condition: service_healthy
#      samm-collector:
#        condition: service_started
#    restart: on-failure:3
#    volumes:
#      - ./config/samm:/usr/local/samm/etc
#
#  samm-collector:
#    build: ./sammbase
#    command: [ "/usr/local/bin/collector.py", "/usr/local/samm/etc/conf.json" ]
#    env_file:
#      - ./config/env/samm.env
#    deploy:
#      mode: replicated
#      replicas: 0
#    depends_on:
#      mq:
#        condition: service_healthy
#      mimir:
#        condition: service_started
#      prometheus:
#        condition: service_started
#    restart: on-failure:3
#    volumes:
#      - ./config/samm:/usr/local/samm/etc

volumes:
  minio-data:
  mimir-data:
  mysql-data:
  prometheus-data:
